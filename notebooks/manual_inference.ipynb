{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters, morphology\n",
    "from rasterio import features \n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point\n",
    "from shapely.geometry import Point\n",
    "import shapely.ops as ops\n",
    "from shapely.ops import cascaded_union\n",
    "\n",
    "from shapely import geometry\n",
    "import itertools\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import ntpath\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import plotting_extent\n",
    "import geopandas as gpd\n",
    "import earthpy as et\n",
    "import earthpy.spatial as es\n",
    "import earthpy.plot as ep\n",
    "\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial import distance\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from gbdxtools import Interface\n",
    "from fiona.crs import from_epsg\n",
    "gbdx = Interface()\n",
    "from keras.models import load_model\n",
    "\n",
    "import rasterio\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Model And Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_selection = range(17)\n",
    "\n",
    "property_names = ['cloudCover',\n",
    "                 'multiResolution',\n",
    "                 'targetAzimuth',\n",
    "                 # 'timestamp',\n",
    "                 'sunAzimuth',\n",
    "                 'offNadirAngle',\n",
    "                 # 'platformName',\n",
    "                 'sunElevation',\n",
    "                 # 'scanDirection',\n",
    "                 'panResolution']\n",
    "\n",
    "save_classification = True\n",
    "\n",
    "# Create function that reshapes numpy arrays\n",
    "def reshape(index):\n",
    "    index = np.reshape(index,[1,index.shape[0],index.shape[1]])\n",
    "    return(index)\n",
    "\n",
    "def rgb_from_raster(data, brightness):\n",
    "    \n",
    "    bands, x, y = data.shape\n",
    "    \n",
    "    # create plottable image\n",
    "    brightness = 0.3\n",
    "    blue = data[1].astype(np.float32)\n",
    "    green = data[2].astype(np.float32)\n",
    "    red = data[4].astype(np.float32)\n",
    "    rgb = np.zeros((x,y,3))\n",
    "    rgb[...,0] = red\n",
    "    rgb[...,1] = green\n",
    "    rgb[...,2] = blue\n",
    "    rgb = rgb - np.min(rgb)\n",
    "    rgb = rgb / np.maximum(np.max(rgb), 1) + brightness\n",
    "    \n",
    "    if rgb.mean() > 1:\n",
    "        rgb[rgb > 255] = 255\n",
    "    else: \n",
    "        rgb[rgb > 1] = 1\n",
    "    \n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_inference(model_weight_path = None, raster_files_path = None, output_file_path = 'classification/'):\n",
    "    path = Path(raster_files_path)\n",
    "    if raster_files_path is not None \\\n",
    "        and os.path.exists(path.parent) \\\n",
    "        and model_weight_path is not None \\\n",
    "        and os.path.exists(model_weight_path):\n",
    "        model = load_model(model_weight_path)\n",
    "        \n",
    "        raster_files = glob(raster_files_path)\n",
    "        if len(raster_files) == 1:\n",
    "            sample_size = 1\n",
    "        else:\n",
    "            sample_size = 3\n",
    "        for file_path in np.array(pd.DataFrame(raster_files).sample(sample_size)).flatten(): \n",
    "    \n",
    "            print(file_path)\n",
    "            filename = file_path.split('/')[-1]\n",
    "\n",
    "            data = rasterio.open(file_path).read()\n",
    "            dataset = rasterio.open(file_path)\n",
    "\n",
    "            transform = dataset.meta['transform']\n",
    "            width = dataset.meta['width']\n",
    "            height = dataset.meta['height']\n",
    "\n",
    "            # Split image data into band layers\n",
    "            coastal = data[0] \n",
    "            blue    = data[1]\n",
    "            green   = data[2]\n",
    "            yellow  = data[3]\n",
    "            red     = data[4] \n",
    "            red_edge = data[5] \n",
    "            nir1 = data[6]\n",
    "            nir2 = data[7]\n",
    "\n",
    "            # get info from filenames\n",
    "            stringlist = file_path.split('/')[-1].split('_')\n",
    "            image_id = stringlist[-1].split('.')[0]\n",
    "\n",
    "            # get metadata from GBDX\n",
    "            record = gbdx.catalog.get(image_id)\n",
    "\n",
    "            # add metadata to data \n",
    "            i = 8\n",
    "            for property_name in property_names:\n",
    "\n",
    "                    try: \n",
    "                        property_record = record['properties'][property_name]  \n",
    "\n",
    "                        property_array = np.empty([1,data.shape[1],data.shape[2]])\n",
    "                        property_array.fill(property_record)\n",
    "\n",
    "                        data = np.vstack([data,property_array])\n",
    "                    except: \n",
    "                        print('failed ', property_name, image_id)\n",
    "                        property_array = np.empty([1,data.shape[1],data.shape[2]])\n",
    "                        data = np.vstack([data,property_array])\n",
    "\n",
    "                    i = i + 1\n",
    "\n",
    "\n",
    "            # Calculate indices\n",
    "            # Make sure ndvi and ndwi are stackable with band data by making them 3-dimensional (1,x,y)\n",
    "            ndvi = reshape((nir1 - red)/(nir1 + red))\n",
    "            ndwi = reshape((green - nir1) / (green + nir1))\n",
    "            wwi = reshape((coastal - nir2) / (coastal + nir2 + 10e-5))\n",
    "\n",
    "            # Stack arrays in sequence vertically (row wise)\n",
    "            image_array = np.vstack([data,ndvi,ndwi, wwi])\n",
    "            # select all variables of interest\n",
    "            image_array = image_array[variable_selection,:,:]\n",
    "\n",
    "            ### Reshape the data prior to feeding it through the model.\n",
    "            # Flatten the two-dimensional imagery (i.e., Reshape)\n",
    "            d2, h2, w2 = image_array.shape\n",
    "            data_pred = np.reshape(image_array,(d2,h2*w2)).T\n",
    "                \n",
    "            # Convert data type to 32-bit float, and normalize between 0 and 1\n",
    "            data_pred.astype(np.float32)\n",
    "            \n",
    "            min_data =  np.nanmin(data_pred)\n",
    "            max_data = np.nanmax(data_pred)\n",
    "            \n",
    "            data_pred = (data_pred - min_data) / (max_data - min_data)\n",
    "\n",
    "            # Next, conduct model prediction on each spectral column\n",
    "            labels_pred = model.predict(data_pred, verbose = 1, workers = 4)\n",
    "            labels_pred = np.reshape(labels_pred,(h2,w2))\n",
    "\n",
    "            ### Calculate threshold for classification  \n",
    "            thresh = filters.threshold_otsu(labels_pred)\n",
    "\n",
    "            ### Plot the input image alongside its corresponding predictions.\n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.subplot(131);plt.imshow(rgb_from_raster(data, 0.0)); plt.title(filename)\n",
    "            plt.subplot(132);plt.imshow(labels_pred, cmap = 'Blues');\n",
    "            # plt.colorbar()\n",
    "            plt.subplot(133);plt.imshow(labels_pred > 0.4, cmap = 'Blues');\n",
    "            plt.draw()\n",
    "            plt.show()\n",
    "\n",
    "            ### save classifications to raster\n",
    "            labels_pred = labels_pred.astype(float)\n",
    "\n",
    "            array_classification = labels_pred > thresh\n",
    "            array_classification = array_classification.astype(float)\n",
    "\n",
    "            classification_filename = output_file_path + 'classification_' + filename\n",
    "\n",
    "            if save_classification:\n",
    "                with rasterio.open(\n",
    "                    classification_filename,\n",
    "                    'w',\n",
    "                    driver='GTiff',\n",
    "                    height=labels_pred.shape[0],\n",
    "                    width=labels_pred.shape[1],\n",
    "                    count=1,\n",
    "                    dtype=labels_pred.dtype,\n",
    "                    crs='+proj=latlong',\n",
    "                    transform=transform,\n",
    "                ) as dst:\n",
    "                    dst.write(labels_pred, 1)\n",
    "\n",
    "\n",
    "                print('saved as: ', classification_filename)\n",
    "    else:\n",
    "        print('There is no model weight or something is wrong with the path')\n",
    "model_weight_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/model_weight/bfalg_batchSize_200_Epoch-00185-Val_acc-0.95145_weights.hdf5'\n",
    "raster_files_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/img_data_to_combine/*.tif'\n",
    "output_file_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/raster_classifications/'\n",
    "\n",
    "# run_the_inference(model_weight_path, raster_files_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize The Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_linestrings(array, mask, affine):\n",
    "    \n",
    "    if np.all(mask) == True:\n",
    "        final_lines = []\n",
    "    else:\n",
    "        # Convert the array to polygon geometries\n",
    "        polygon_generator = features.shapes(array.astype('uint8'), mask=array, transform=affine)\n",
    "        # Extract out the individual polygons, fixing any invald geometries using buffer(0)\n",
    "        polygons = [shape(g).buffer(0) for g,v in polygon_generator]\n",
    "\n",
    "        # Next, build up a binary array representing the outer boundary of the input array\n",
    "        boundary_array = np.ones(array.shape)    \n",
    "\n",
    "        # Convert the boundary to a polygon too\n",
    "        boundary_generator = features.shapes(boundary_array.astype('uint8'), transform=affine)\n",
    "        boundary = [shape(g) for g,v in boundary_generator][0]\n",
    "\n",
    "        # Convert the mask to a polygon too\n",
    "        if mask is not None:\n",
    "            \n",
    "            mask_generator = features.shapes(mask.astype('uint8'), \n",
    "                                             mask= mask > 0,\n",
    "                                             transform=affine)\n",
    "            mask_boundary = [shape(g).buffer(0) for g,v in mask_generator][0]\n",
    "\n",
    "        # Convert the array polygons to linestrings\n",
    "        exterior_rings = [g.exterior for g in polygons]\n",
    "        interior_rings = list(itertools.chain.from_iterable([[i for i in g.interiors] for g in polygons if len(g.interiors) > 0]))\n",
    "        all_rings =  exterior_rings + interior_rings \n",
    "\n",
    "        # Erase both the image boundary and the mask boundary from the extracted Linestrings\n",
    "        erased_lines = all_rings #[g.difference(boundary.exterior).difference(mask_boundary.exterior) for g in all_rings]\n",
    "        singlepart_lines = [list(g) if type(g) == geometry.multilinestring.MultiLineString else [g] for g in erased_lines ]\n",
    "        final_lines = list(itertools.chain(*singlepart_lines))\n",
    "\n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_water_bodies_polygon(inference_img_file_path=None):\n",
    "    \n",
    "    if inference_img_file_path is not None \\\n",
    "        and os.path.exists(inference_img_file_path):\n",
    "        \n",
    "        inference_data = rasterio.open(inference_img_file_path).read()\n",
    "        inference_dataset = rasterio.open(inference_img_file_path)\n",
    "\n",
    "        affine = inference_dataset.meta['transform']\n",
    "        width = inference_dataset.meta['width']\n",
    "        height = inference_dataset.meta['height']\n",
    "        \n",
    "        thresh = filters.threshold_otsu(inference_data[0])\n",
    "        data_otsu_treshold = inference_data[0] > thresh\n",
    "        water_data = data_otsu_treshold.astype('uint8')\n",
    "        \n",
    "        # rescale values to 0 - 255 to use image processing algorithms\n",
    "        OldRange = (water_data.max() - water_data.min())  \n",
    "        NewRange = (0 - 255)  \n",
    "        NewValue = (((water_data - water_data.min()) * NewRange) / OldRange) + 0\n",
    "\n",
    "        water_data = NewValue.round().astype('uint8')\n",
    "\n",
    "        water_blurred = cv.medianBlur(water_data,15)\n",
    "\n",
    "        # Specify the minimum feature size in square meters, and then use info from the image metadata \n",
    "        # to translate to grid cell count.\n",
    "\n",
    "        # Set the minimum feature size to 400 sq km (400,000 m)\n",
    "        min_feature_size_m2 = 500.\n",
    "\n",
    "        # From the image metadata, we can determine the area of a single grid cell\n",
    "        try: \n",
    "            cell_height_m = image.rda.metadata['image']['groundSampleDistanceMeters']\n",
    "        except: \n",
    "            cell_height_m = 0.40\n",
    "\n",
    "        cell_area_m2 = cell_height_m**2\n",
    "\n",
    "        # Finally, use the cell size to convert the minimum feature size to grid cells\n",
    "        min_feature_size_cells = np.round((min_feature_size_m2/cell_area_m2)).astype('int64')\n",
    "\n",
    "        # What is the minimum feature size in square meters and grid cells?\n",
    "        print(min_feature_size_m2, 'sq. m')\n",
    "        print(min_feature_size_cells, 'grid cells')\n",
    "\n",
    "        # First, remove the small holes\n",
    "        water_cleaned = morphology.remove_small_holes(water_blurred, min_feature_size_cells)\n",
    "\n",
    "        # Then remove the small objects\n",
    "        water_cleaned = morphology.remove_small_objects(water_cleaned, min_feature_size_cells, connectivity=2)\n",
    "        \n",
    "        # View the results\n",
    "        plt.figure(figsize=(20,15))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(inference_data[0], vmin=0, vmax=1, cmap = 'Blues')\n",
    "        plt.title(\"Water Mask\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(water_cleaned, vmin=0, vmax=1, cmap = 'Blues')\n",
    "        plt.title(\"Cleaned Water Mask\")\n",
    "        print(water_cleaned.shape)\n",
    "        \n",
    "        water_lines = array_to_linestrings(array = water_cleaned, mask = water_cleaned, affine = affine)\n",
    "\n",
    "        # Run a basic simplification algorithm to smooth the lines\n",
    "        water_lines_smooth = [g.simplify(0.001) for g in water_lines]\n",
    "\n",
    "        # GLue it all together\n",
    "        detection = cascaded_union(water_lines)\n",
    "        \n",
    "        detection_poly = ops.polygonize_full(detection)\n",
    "\n",
    "        detection_poly = cascaded_union(detection_poly)\n",
    "\n",
    "        buffer = 0# 1e-05\n",
    "\n",
    "        detection_poly_smooth = detection_poly.buffer(-buffer).buffer(buffer)\n",
    "        detection_poly = ops.polygonize_full(detection_poly_smooth)[0]\n",
    "        return detection_poly\n",
    "\n",
    "inference_img_file_path ='/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/img_data_to_combine/Setu_mangga_bolong_2019-05-31_03_28_104001004CBD1F00.tif'    \n",
    "inference_img_file_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/raster_classifications/classification_Setu_Babakan_N_2019-05-31_03_28_104001004CBD1F00.tif'\n",
    "# res = get_detected_water_bodies_polygon(inference_img_file_path)\n",
    "# res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Over Satellite Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_overlayed_on_rgb_img(polygon=None, raster_image_path=None):\n",
    "    if os.path.exists(raster_image_path) \\\n",
    "        and polygon is not None:\n",
    "        raster_img = rasterio.open(raster_image_path)\n",
    "        raster_profile = raster_img.profile\n",
    "        bound = gpd.GeoSeries(polygon)\n",
    "        bound = gpd.GeoDataFrame({'geometry': bound}, index=[0],  crs=from_epsg(4326))\n",
    "\n",
    "        bound_utm13N = bound.to_crs(raster_profile[\"crs\"])\n",
    "        extent = plotting_extent(raster_img.read()[0], raster_profile[\"transform\"])\n",
    "\n",
    "        # Create figure with one plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "        # Plot boundary with high zorder for contrast\n",
    "        bound_utm13N.boundary.plot(ax=ax, color=\"red\", zorder=10)\n",
    "\n",
    "        # Plot CIR image using the raster extent\n",
    "        ep.plot_rgb(\n",
    "            raster_img.read(),\n",
    "            rgb=(4, 2, 1),\n",
    "            ax=ax,\n",
    "            stretch=True,\n",
    "            extent=extent,\n",
    "            str_clip=0.5,\n",
    "            title=\"Water Body Detection\",\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"The path does not exist or there is none polygon object\")\n",
    "\n",
    "# res = get_detected_water_bodies_polygon(inference_img_file_path)\n",
    "raster_image_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/img_data_to_combine/Setu_Babakan_N_2019-05-31_03_28_104001004CBD1F00.tif'\n",
    "# plot_boundary_overlayed_on_rgb_img(polygon=res[0], raster_image_path=raster_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgs2epsgzone(x,y):\n",
    "    EPSG = 32700-round((45+y)/90,0)*100+round((183+x)/6,0)\n",
    "    UTM_EPSG_code = EPSG\n",
    "    \n",
    "    return UTM_EPSG_code\n",
    "\n",
    "def polygon_raster_overlap(poly, n_closest, raster_files):    \n",
    "    for n in n_closest:     \n",
    "\n",
    "        # load raster data\n",
    "        dataset = rasterio.open(raster_files[n])\n",
    "\n",
    "        # convert bounds to polygon\n",
    "        left, bottom, right, top = dataset.bounds\n",
    "        polygon = Polygon([(left, bottom), (left, top), (right, top), (right, bottom)])\n",
    "\n",
    "        # calculate overlap fraction\n",
    "        overlap_area = round(poly.intersection(polygon).area / poly.area, 2)\n",
    "\n",
    "        if overlap_area == 1:\n",
    "            break  \n",
    "    \n",
    "    return n\n",
    "\n",
    "def random_points_within(poly, n_points_per_sqm):\n",
    "    min_x, min_y, max_x, max_y = poly.bounds\n",
    "    \n",
    "    epsg = wgs2epsgzone(max_x, max_y)\n",
    "    \n",
    "    project = partial(\n",
    "        pyproj.transform,\n",
    "        pyproj.Proj(init='epsg:4326'),\n",
    "        pyproj.Proj(init='epsg:%i'  % (epsg)))\n",
    "\n",
    "\n",
    "    poly_wgs = transform(project, poly)\n",
    "    \n",
    "    n_points = int((poly_wgs.area / 1e6) * n_points_per_sqm)\n",
    "    \n",
    "    if n_points > 10000:\n",
    "        n_points = 10000\n",
    "    \n",
    "    if n_points < 10:\n",
    "        n_points = 50\n",
    "    \n",
    "#     print('area ' + str(poly_wgs.area))\n",
    "#     print('n_points ' + str(n_points))\n",
    "\n",
    "    points = []\n",
    "\n",
    "    while len(points) < n_points:\n",
    "        random_point = Point([random.uniform(min_x, max_x), random.uniform(min_y, max_y)])\n",
    "        if (random_point.within(poly)):\n",
    "            points.append(random_point)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_values_for_points(dataset, points):\n",
    "\n",
    "    bands_save = []\n",
    "    x_save = []\n",
    "    y_save = []\n",
    "    \n",
    "    data_array = dataset.read()\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        x = points[i].x\n",
    "        y = points[i].y\n",
    "\n",
    "        x_save.append(x)\n",
    "        y_save.append(y)\n",
    "\n",
    "        index = dataset.index(x, y)\n",
    "\n",
    "        try: \n",
    "            band_values = data_array[:,index[0],index[1]]\n",
    "            bands_save.append(band_values)\n",
    "        except:\n",
    "            # print('point ' + str(i) + ' out of image')\n",
    "            continue\n",
    "\n",
    "#         print(band_values)\n",
    "        \n",
    "    return bands_save, x_save, y_save\n",
    "\n",
    "def get_data_raster_polygons(class_MultiPolygon, label, variables, image_locations, raster_files, plotting_overlap = False):\n",
    "    \n",
    "    nr_of_polygons = len(class_MultiPolygon)\n",
    "    n_points_per_sqkm = 5000\n",
    "    # create dataframe with all variables + label\n",
    "    variables_str_list= variables + ['label']\n",
    "    df_subsample = pd.DataFrame(columns = variables_str_list)\n",
    "\n",
    "    # len(df_polygon_image_id)\n",
    "    for i in tqdm(range(nr_of_polygons)):\n",
    "\n",
    "        # select polygon\n",
    "        poly = class_MultiPolygon[i]\n",
    "\n",
    "        # get polygon location as shapely obj.\n",
    "        [x,y] = poly.centroid.xy\n",
    "        point = np.array([x[0],y[0]])\n",
    "\n",
    "        # get index of closest 5 images to polygon\n",
    "        n_closest = distance.cdist([point], image_locations).argsort()[0,0:5]\n",
    "\n",
    "        # check overlap for each image agianst the polygon\n",
    "        n_maxoverlap = polygon_raster_overlap(poly, n_closest, raster_files)\n",
    "\n",
    "        # get raster corresponding to polygon\n",
    "        path_match_image = raster_files[n_maxoverlap]\n",
    "        \n",
    "        # create random samples in polygon\n",
    "        samples = random_points_within(poly, n_points_per_sqkm)\n",
    "        \n",
    "        df = pd.DataFrame(np.zeros([len(samples),len(variables)]),  columns = variables)\n",
    "\n",
    "        # get band values for random samples\n",
    "        dataset = rasterio.open(path_match_image)\n",
    "        bands_save, x_save, y_save = get_values_for_points(dataset, samples)\n",
    "\n",
    "        # visualize training data selection\n",
    "        try: \n",
    "            if plotting_overlap:\n",
    "                # clear_output(wait = True)\n",
    "\n",
    "                # get boundary xy coordinates for plotting\n",
    "                try:       \n",
    "                    if len(poly.boundary) == 1:\n",
    "                        x,y = poly.boundary.xy\n",
    "                    else:\n",
    "                        n_largest = np.array([boundary.length for boundary in poly.boundary]).argmax()\n",
    "                        x,y = poly.boundary[n_largest].xy\n",
    "                except: \n",
    "                    x,y = poly.boundary.xy                \n",
    "\n",
    "                data = dataset.read()\n",
    "                data = data - np.min(data)\n",
    "                data = data / np.maximum(np.max(data), 1) + 0.3\n",
    "\n",
    "                plt.figure(figsize = (10,10))\n",
    "                plt.plot(x,y, color = 'w')\n",
    "                for point in samples:\n",
    "                    plt.scatter(point.x,point.y, c = 'red', marker = '.')\n",
    "                show(data[[1,3,4],:,:], transform=dataset.transform)\n",
    "        except Exception as e:\n",
    "            print('failed')\n",
    "            print('poly nr: ', i, ' image: ', path_match_image)\n",
    "            print(e)\n",
    "            print('___________________________________________\\n')\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        try:\n",
    "            df.loc[:,0:8] = bands_save\n",
    "            # print('data', start_row, end_row)\n",
    "        except: \n",
    "            print('no data', path_match_image)\n",
    "            continue\n",
    "\n",
    "        # get image_id from filename\n",
    "        stringlist = path_match_image.split('/')[-1].split('_')\n",
    "        image_id = stringlist[-1].split('.')[0]\n",
    "\n",
    "        # get metadata \n",
    "        record = gbdx.catalog.get(image_id)\n",
    "        df.loc[:,'x'] = x_save\n",
    "        df.loc[:,'y'] = y_save\n",
    "\n",
    "        for property_name in property_names:\n",
    "\n",
    "            try: \n",
    "                property_record = record['properties'][property_name]    \n",
    "                df[property_name] = property_record\n",
    "            except: \n",
    "                print('failed ', property_name, image_id)\n",
    "                df[property_name] = None\n",
    "\n",
    "        df['label'] = label\n",
    "\n",
    "        df_subsample = df_subsample.append(df, ignore_index=True)\n",
    "    \n",
    "    return df_subsample\n",
    "\n",
    "def raster_path_list2image_centroid_coordinate(raster_path_list):\n",
    "    \n",
    "    image_locations = np.zeros([len(raster_path_list), 2])\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for path in tqdm(raster_path_list):\n",
    "\n",
    "        dataset = rasterio.open(path)\n",
    "\n",
    "        # get info from filenames\n",
    "        stringlist = path.split('/')[-1].split('_')\n",
    "        seq_nr = stringlist[1]\n",
    "        image_id = stringlist[-1].split('.')[0]\n",
    "        city = stringlist[0]\n",
    "\n",
    "        # calculate center of image \n",
    "        left, bottom, right, top = dataset.bounds\n",
    "        x_center = (left + right) / 2\n",
    "        y_center = (top + bottom) / 2\n",
    "\n",
    "        image_locations[i,0] = x_center\n",
    "        image_locations[i,1] = y_center\n",
    "\n",
    "\n",
    "        i = 1 + i\n",
    "    \n",
    "    return image_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input(polygon=None, raster_image_path=None):\n",
    "    plot_boundary_overlayed_on_rgb_img(polygon=polygon, raster_image_path=raster_image_path)\n",
    "    user_input = input('Is it water?: y/n\\n')\n",
    "    return user_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_user_input(polygon=None, raster_image_path=None, label=None):\n",
    "    count = 0\n",
    "    property_names = ['cloudCover',\n",
    "                 'multiResolution',\n",
    "                 'targetAzimuth',\n",
    "                 # 'timestamp',\n",
    "                 'sunAzimuth',\n",
    "                 'offNadirAngle',\n",
    "                 # 'platformName',\n",
    "                 'sunElevation',\n",
    "                 # 'scanDirection',\n",
    "                 'panResolution']\n",
    "\n",
    "    variables = [0, 1, 2, 3, 4, 5, 6, 7, 'x', 'y'] + property_names\n",
    "    image_locations = raster_path_list2image_centroid_coordinate([raster_image_path])\n",
    "    df =  get_data_raster_polygons([polygon],\n",
    "                                       label,\n",
    "                                      variables,\n",
    "                                      image_locations,\n",
    "                                      [raster_image_path],\n",
    "                                      plotting_overlap = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function To Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_manual_inference():\n",
    "    count = 0\n",
    "    training_dataframe = pd.DataFrame()\n",
    "    n_points_per_sqkm = 5000\n",
    "    model_weight_path = input('Enter the model weight path\\n').strip()\n",
    "    raster_files_path = input('Enter the path containing the input tif files\\n')\n",
    "    output_file_path = input('Enter the path to store the inference tif files\\n')\n",
    "    \n",
    "    run_the_inference(model_weight_path, raster_files_path, output_file_path)\n",
    "    \n",
    "    inference_img_file_path_list = glob(output_file_path + '*.tif')\n",
    "    print(inference_img_file_path_list)\n",
    "    #select randomly 3 files for now\n",
    "    inference_img_file_path_list = random.sample(inference_img_file_path_list, 3)\n",
    "    \n",
    "    for inference_img_file_path in inference_img_file_path_list:\n",
    "    \n",
    "        detection_poly = get_detected_water_bodies_polygon(inference_img_file_path)\n",
    "        count+=1\n",
    "        for poly in detection_poly:\n",
    "            filename = ntpath.basename(inference_img_file_path)\n",
    "            path = Path(raster_files_path)\n",
    "            \n",
    "            filename = filename.replace('classification_','')\n",
    "            raster_image_path = str(path.parent) + '/' + filename\n",
    "            user_input = get_user_input(poly, raster_image_path)\n",
    "            if user_input == 'y':\n",
    "                label = 'water'\n",
    "                training_dataframe = pd \\\n",
    "                                        .concat([training_dataframe,store_user_input(poly, raster_image_path,label)])\n",
    "            else:\n",
    "                label = 'non_water'\n",
    "                training_dataframe = pd \\\n",
    "                                        .concat([training_dataframe,store_user_input(poly, raster_image_path,label)])\n",
    "    output_path_to_store_latest_training_dataset = input('Enter the path to store the manual inference output')\n",
    "    output_path_to_store_latest_training_dataset += '/user_output_' + str(count) + '.csv' \n",
    "    training_dataframe.to_csv(output_path_to_store_latest_training_dataset, index=False)\n",
    "\n",
    "# main()\n",
    "model_weight_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/pre_trained_model_weights/bfalg_batchSize_200_Epoch-00185-Val_acc-0.95145_weights.hdf5'\n",
    "raster_files_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/img_data_to_combine/*.tif'\n",
    "output_file_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/raster_classifications/'\n",
    "inference_img_file_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/raster_classifications/classification_Setu_Babakan_N_2019-05-31_03_28_104001004CBD1F00.tif'\n",
    "manual_inference_path = '/home/daffolap-851/Projects/GreenCityWatch/manual_inference_data_and_weights/pixel_based_training_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-green_city_watch_dev] *",
   "language": "python",
   "name": "conda-env-.conda-green_city_watch_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
